# âš™ï¸ ClarifyCoder-Agent â€“ Backend

<p align="center">
  <img src="./docs/architecture.png" alt="ClarifyCoder Architecture" width="800"/>
</p>

> **ClarifyCoder-Agent** is a **multi-agent system for ambiguity resolution in code generation**.  
> It splits responsibilities across **specialist agents** (Clarify, Answer, Code, Eval, Refine) and introduces new evaluation metrics (ARSR, CRR, CSR, RFR, USR).  
> This backend powers the research pipeline and connects with the frontend UI.

---

## âœ¨ Core Features

- ğŸ”¹ **Multi-Agent Flow** â†’ ClarifyAgent â†’ AnswerAgent â†’ CodeAgent â†’ EvalAgent â†’ RefineAgent
- ğŸ”¹ **Dual Implementations** â†’ Baseline (rule-based) and LLM (GPT-4o-mini)
- ğŸ”¹ **Ambiguity Dataset** (`prompts.jsonl`) â†’ ambiguous + clear coding tasks
- ğŸ”¹ **Novel Metrics**: ARSR, CSR, CRR, USR, RFR
- ğŸ”¹ **Self-Repair Loop** â†’ EvalAgent feedback guides RefineAgent for iterative fixes
- ğŸ”¹ **Experiment Runner** â†’ baseline vs LLM vs hybrid comparison
- ğŸ”¹ **Visualization** â†’ Matplotlib plots + Streamlit leaderboard dashboard
- ğŸ”¹ **Human-in-the-Loop Mode** â†’ interactive Q&A vs fully automated LLM answers

---

## ğŸ“‚ Project Structure

```plaintext
clarifycoder-backend/
 â”œâ”€ agentic_clarifycoder/
 â”‚   â”œâ”€ core/
 â”‚   â”‚   â”œâ”€ agents/
 â”‚   â”‚   â”‚   â”œâ”€ answer_agent.py
 â”‚   â”‚   â”‚   â”œâ”€ clarify_agent_llm.py
 â”‚   â”‚   â”‚   â”œâ”€ clarify_agent_rulebased.py
 â”‚   â”‚   â”‚   â”œâ”€ code_agent_llm.py
 â”‚   â”‚   â”‚   â”œâ”€ code_agent_rulebased.py
 â”‚   â”‚   â”‚   â”œâ”€ eval_agent_llm.py
 â”‚   â”‚   â”‚   â”œâ”€ eval_agent_rulebased.py
 â”‚   â”‚   â”‚   â”œâ”€ refine_agent_llm.py
 â”‚   â”‚   â”‚   â””â”€ refine_agent_rulebased.py
 â”‚   â”‚   â”œâ”€ demo/
 â”‚   â”‚   â”‚   â””â”€ demo.py              # CLI demo runner
 â”‚   â”‚   â”œâ”€ utils/
 â”‚   â”‚   â”‚   â””â”€ runner.py            # Logging & helpers
 â”‚   â”‚   â””â”€ __init__.py
 â”‚   â”œâ”€ plots/                       # Generated experiment plots
 â”‚   â”œâ”€ logs/                        # Experiment logs
 â”‚   â”œâ”€ clarify_agent_clarifycoder.py
 â”‚   â”œâ”€ compare_experiments.py       # Experiment runner + plots
 â”‚   â”œâ”€ dashboard.py                 # Streamlit leaderboard
 â”‚   â”œâ”€ prompts.jsonl                # Ambiguity dataset
 â”‚   â””â”€ __init__.py
 â”œâ”€ results.csv / results.xlsx       # Experiment results
 â”œâ”€ app.py                           # FastAPI entrypoint (for frontend integration)
 â”œâ”€ requirements.txt                 # Dependencies
 â”œâ”€ pyproject.toml
 â”œâ”€ Procfile                         # Heroku/Render deployment
 â””â”€ README.md
```
---

## ğŸ§© Agent Responsibilities

| Agent         | Function                                   | Baseline Implementation | LLM Implementation |
|---------------|--------------------------------------------|--------------------------|---------------------|
| **ClarifyAgent** | Detects ambiguous prompts, generates clarifying Qs | Regex/Rule-based        | GPT-4o-mini         |
| **AnswerAgent**  | Provides answers to clarifications       | Human input (CLI/Web)   | GPT-4o-mini         |
| **CodeAgent**    | Generates candidate Python code          | Templates (deterministic) | GPT-4o-mini       |
| **EvalAgent**    | Runs/evaluates code correctness          | Test cases & keyword checks | GPT-4o-mini      |
| **RefineAgent**  | Repairs failed code using feedback       | Simple text rules       | GPT-4o-mini         |

## ğŸ”€ Modes of Operation

| Mode       | ClarifyAgent | CodeAgent | EvalAgent | RefineAgent | AnswerAgent |
|------------|-------------|-----------|-----------|-------------|-------------|
| **Baseline** | Rule-based  | Templates | Test cases & keyword checks | Simple text rules | Human (CLI/Web) |
| **LLM**      | GPT-4o-mini | GPT-4o-mini | GPT-4o-mini | GPT-4o-mini | GPT-4o-mini |
| **Hybrid**   | GPT-4o-mini | Rule-based | Rule-based | GPT-4o-mini | Human/LLM |

## ğŸ“‚ Dataset of Ambiguous Prompts

The system ships with a small benchmark dataset: **`prompts.jsonl`**.

Each line is a JSON object with:
- **prompt** â†’ original ambiguous/clear task  
- **clarifications** â†’ clarification questions generated  
- **solution** â†’ expected clarified solution  
- **failure_mode** â†’ expected incorrect outcome if clarification is skipped  

ğŸ“Œ Example entry:
```json
{
  "id": 1,
  "prompt": "Find depth of binary tree",
  "clarifications": ["Do you mean depth of the entire tree, or a given node?"],
  "solution": "Function to compute max depth of full binary tree",
  "failure_mode": "If skipped â†’ code may compute depth of only root"
}
```

## ğŸ“Š Evaluation Metrics

| Metric | Formula | Meaning |
|--------|---------|---------|
| **ARSR** | (# resolved ambiguous prompts with correct code) Ã· (total ambiguous prompts) | Ambiguity-Resolved Success Rate |
| **CSR**  | (# correct clear prompts) Ã· (total clear prompts) | Clear Success Rate |
| **CRR**  | (# prompts needing clarification) Ã· (total ambiguous prompts) | Clarification Request Rate |
| **USR**  | (# ambiguous solved without clarification) Ã· (total ambiguous prompts) | Unresolved Success Rate |
| **RFR**  | (# successful refinements) Ã· (total refinement attempts) | Refinement Fix Rate |

---

## ğŸš€ Running the System

### 1. Install Dependencies
```bash
git clone https://github.com/hsb-amjad/clarifycoder-backend.git
cd clarifycoder-backend
pip install -r requirements.txt
```

### 2. Run Demo
```bash
# Baseline-only agents
python -m agentic_clarifycoder.core.demo.demo --clarify_mode baseline --code_mode baseline --eval_mode baseline --refine_mode baseline --answer_mode human

# LLM-only agents
python -m agentic_clarifycoder.core.demo.demo --clarify_mode llm --code_mode llm --eval_mode llm --refine_mode llm --answer_mode auto
```
- answer_mode can be switched between auto and HIL.

### 3. Run Comparison-Experiments
```bash
cd agentic_clarifycoder
python compare_experiments.py --n_prompts 10 --runs 3 --answer_mode auto
```
#### Args:
```lua
--n_prompts   Number of prompts (default: random 5â€“10)
--runs        runs per mode
```
- answer_mode can be switched between auto and HIL.
#### Generates:
- ğŸ“Š results.csv and results.xlsx
- ğŸ“ˆ Plots in /plots
- ğŸ“ Logs in /logs/<timestamp>/

### 4. Streamlit Dashboard
```bash
streamlit run dashboard.py
```
Interactive leaderboard with metrics, plots, and raw log inspection.

---

## ğŸ“Š Example Outputs

### ğŸ“ˆ Bar Chart of Metrics (Baseline vs LLM vs Hybrid)
<p align="center">
  <img src="./agentic_clarifycoder/plots/metrics_bar.png" alt="Metrics Bar Chart" width="700"/>
</p>

### ğŸ“‰ Trend Plots for ARSR
<p align="center">
  <img src="./agentic_clarifycoder/plots/llm_trend.png" alt="LLM ARSR Trend" width="600"/>
</p>

### ğŸ–¥ï¸ Dashboard
<p align="center">
  <img src="./docs/streamlit.png" alt="Streamlit Leaderboard" width="800"/>
</p>

---

## ğŸ”¬ Research Contribution

- ğŸ“‘ Novelty: First ambiguity-aware benchmark for code generation
- ğŸ“Š Metrics: ARSR introduced + supporting metrics for clarity, refinement, unresolved cases
- ğŸ”„ Self-repair loop: Evaluation â†’ refinement cycle for robustness
- ğŸ‘¤ Human-in-the-Loop: Compare human clarifications vs auto-LLM clarifications
- ğŸ“ˆ Evaluation: Supports HumanEval + MBPP + custom ambiguity dataset
- ğŸ¯ Hybrid configuration: Best balance between reproducibility (rules) and flexibility (LLMs)

---

## ğŸ“š Citation

If you use ClarifyCoder-Agent in academic work:
```bibtex
@misc{clarifycoder2025,
  title={ClarifyCoder-Agent: Multi-Agent Coding Assistant with Ambiguity Resolution},
  author={Amjad, Haseeb},
  year={2025},
  howpublished={GitHub},
}
```

## â­ Support

If this project helps your research, please star â­ the repo.
Contributions and feedback are welcome!

## ğŸ‘¤ Author

**Haseeb Amjad** â€“ Mechatronics Engineer | Machine Learning | AI + Robotics | MedTech  
ğŸŒ [Portfolio](https://my-portfolio-sage-zeta-79.vercel.app)
ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/hsb-amjad)
